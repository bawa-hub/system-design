# Lesson 8: Performance Tuning and Optimization

## üéØ Learning Objectives
By the end of this lesson, you will understand:
- Systematic approach to Kafka performance tuning
- Throughput optimization strategies
- Latency reduction techniques
- Resource utilization optimization
- Scaling strategies and capacity planning
- Performance testing and benchmarking
- Production performance best practices

## üìö Theory Section

### 1. **Performance Tuning Methodology**

#### **The Performance Tuning Process:**
1. **Measure Baseline**: Establish current performance metrics
2. **Identify Bottlenecks**: Find the limiting factors
3. **Apply Changes**: Make targeted optimizations
4. **Measure Impact**: Compare before and after
5. **Iterate**: Continue until goals are met

#### **Key Performance Metrics:**
- **Throughput**: Messages/bytes per second
- **Latency**: End-to-end message processing time
- **Resource Usage**: CPU, memory, disk, network
- **Error Rate**: Failed operations percentage
- **Consumer Lag**: Messages behind latest offset

### 2. **Throughput Optimization**

#### **Producer Throughput Tuning:**

**Batching Configuration:**
```go
config.Producer.Flush.Frequency = 10 * time.Millisecond
config.Producer.Flush.Bytes = 1024 * 1024 // 1MB
config.Producer.Flush.Messages = 1000
```

**Compression Optimization:**
```go
// For high throughput, use LZ4 or Snappy
config.Producer.Compression = sarama.CompressionLZ4
// For maximum compression, use GZIP
config.Producer.Compression = sarama.CompressionGZIP
```

**Acknowledgment Strategy:**
```go
// For maximum throughput, use acks=1
config.Producer.RequiredAcks = sarama.WaitForLocal
// For durability, use acks=all
config.Producer.RequiredAcks = sarama.WaitForAll
```

#### **Consumer Throughput Tuning:**

**Fetch Configuration:**
```go
config.Consumer.Fetch.Default = 10 * 1024 * 1024 // 10MB
config.Consumer.Fetch.Max = 50 * 1024 * 1024 // 50MB
config.Consumer.MaxWaitTime = 500 * time.Millisecond
```

**Parallel Processing:**
```go
// Increase partition count for more parallelism
// Match consumer count to partition count
// Use multiple consumer groups for different processing
```

#### **Broker Throughput Tuning:**

**JVM Settings:**
```bash
# Heap size (recommended: 6GB for 32GB RAM)
-Xmx6g -Xms6g

# Garbage collection
-XX:+UseG1GC
-XX:MaxGCPauseMillis=20
-XX:InitiatingHeapOccupancyPercent=35
```

**Kafka Configuration:**
```properties
# Number of network threads
num.network.threads=8

# Number of I/O threads
num.io.threads=16

# Socket send/receive buffer
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400

# Log segment size
log.segment.bytes=1073741824

# Log retention
log.retention.hours=168
```

### 3. **Latency Reduction**

#### **Producer Latency Optimization:**

**Reduce Batching:**
```go
config.Producer.Flush.Frequency = 1 * time.Millisecond
config.Producer.Flush.Bytes = 1024 // 1KB
config.Producer.Flush.Messages = 1
```

**Use Async Producer:**
```go
producer, err := sarama.NewAsyncProducer(brokers, config)
// Process success/error channels asynchronously
```

**Optimize Compression:**
```go
// Use Snappy for low latency
config.Producer.Compression = sarama.CompressionSnappy
```

#### **Consumer Latency Optimization:**

**Reduce Fetch Wait Time:**
```go
config.Consumer.MaxWaitTime = 50 * time.Millisecond
config.Consumer.Fetch.Min = 1
```

**Optimize Processing:**
```go
// Process messages in parallel
go func() {
    for message := range messageChannel {
        processMessage(message)
    }
}()
```

#### **Network Latency Optimization:**

**Tune Socket Buffers:**
```properties
socket.send.buffer.bytes=65536
socket.receive.buffer.bytes=65536
```

**Optimize Network Threads:**
```properties
num.network.threads=4
num.io.threads=8
```

### 4. **Resource Utilization Optimization**

#### **Memory Optimization:**

**Producer Memory:**
```go
config.Producer.Flush.Bytes = 512 * 1024 // 512KB
config.Producer.Flush.Messages = 100
config.ChannelBufferSize = 256
```

**Consumer Memory:**
```go
config.Consumer.Fetch.Default = 1024 * 1024 // 1MB
config.ChannelBufferSize = 256
```

**JVM Memory Settings:**
```bash
# Heap size based on available memory
-Xmx4g -Xms4g

# Off-heap memory for page cache
# Let OS handle page cache (default)
```

#### **CPU Optimization:**

**Compression Choice:**
```go
// LZ4: Low CPU, good compression
config.Producer.Compression = sarama.CompressionLZ4

// Snappy: Medium CPU, fast compression
config.Producer.Compression = sarama.CompressionSnappy

// GZIP: High CPU, best compression
config.Producer.Compression = sarama.CompressionGZIP
```

**Thread Configuration:**
```properties
# Match to CPU cores
num.network.threads=4
num.io.threads=8
```

#### **Disk I/O Optimization:**

**Log Configuration:**
```properties
# Use fast SSDs
log.dirs=/fast/ssd/kafka-logs

# Optimize segment size
log.segment.bytes=1073741824

# Batch writes
log.flush.interval.messages=10000
log.flush.interval.ms=1000
```

### 5. **Scaling Strategies**

#### **Horizontal Scaling:**

**Add More Brokers:**
- Increase cluster size
- Redistribute partitions
- Improve fault tolerance

**Add More Partitions:**
- Increase parallelism
- Match consumer count
- Consider rebalancing impact

**Add More Consumers:**
- Scale consumer groups
- Process more messages
- Balance load across consumers

#### **Vertical Scaling:**

**Increase Broker Resources:**
- More CPU cores
- More memory
- Faster disks

**Optimize Configuration:**
- Tune JVM settings
- Optimize Kafka config
- Use faster hardware

#### **Capacity Planning:**

**Throughput Planning:**
```
Required Throughput = Messages per second
Partition Count = Throughput / Messages per partition per second
Consumer Count = Partition Count
Broker Count = (Partition Count * Replication Factor) / Partitions per broker
```

**Storage Planning:**
```
Storage per Broker = (Total Data Size * Replication Factor) / Broker Count
Retention Time = Available Storage / (Data Size per Day)
```

### 6. **Performance Testing and Benchmarking**

#### **Testing Methodology:**

**Load Testing:**
```go
func benchmarkProducer(producer sarama.SyncProducer, messageCount int) {
    start := time.Now()
    for i := 0; i < messageCount; i++ {
        message := &sarama.ProducerMessage{
            Topic: "benchmark-topic",
            Value: sarama.StringEncoder(fmt.Sprintf("Message %d", i)),
        }
        producer.SendMessage(message)
    }
    duration := time.Since(start)
    throughput := float64(messageCount) / duration.Seconds()
    fmt.Printf("Throughput: %.2f messages/second\n", throughput)
}
```

**Latency Testing:**
```go
func measureLatency(producer sarama.SyncProducer) {
    start := time.Now()
    message := &sarama.ProducerMessage{
        Topic: "latency-test",
        Value: sarama.StringEncoder("Latency test message"),
    }
    producer.SendMessage(message)
    latency := time.Since(start)
    fmt.Printf("Latency: %v\n", latency)
}
```

#### **Benchmarking Tools:**

**Kafka Performance Tools:**
```bash
# Producer performance test
kafka-producer-perf-test.sh --topic test-topic --num-records 1000000 --record-size 1024 --throughput 10000 --producer-props bootstrap.servers=localhost:9092

# Consumer performance test
kafka-consumer-perf-test.sh --topic test-topic --bootstrap-server localhost:9092 --messages 1000000
```

**Custom Benchmarking:**
```go
type PerformanceBenchmark struct {
    MessageCount    int
    MessageSize     int
    Duration        time.Duration
    Throughput      float64
    Latency         time.Duration
    ErrorCount      int
}

func (b *PerformanceBenchmark) Run() {
    // Implementation of comprehensive benchmark
}
```

### 7. **Production Performance Best Practices**

#### **Monitoring and Alerting:**

**Key Metrics to Monitor:**
- Producer throughput and latency
- Consumer lag and processing rate
- Broker CPU, memory, disk usage
- Network I/O and errors
- JVM garbage collection metrics

**Alerting Thresholds:**
- Consumer lag > 1000 messages
- CPU usage > 80%
- Memory usage > 85%
- Error rate > 1%
- Latency > 100ms

#### **Configuration Management:**

**Environment-Specific Configs:**
```go
type EnvironmentConfig struct {
    Development  KafkaConfig
    Staging      KafkaConfig
    Production   KafkaConfig
}

func GetConfig(env string) KafkaConfig {
    switch env {
    case "development":
        return getDevelopmentConfig()
    case "staging":
        return getStagingConfig()
    case "production":
        return getProductionConfig()
    default:
        return getDefaultConfig()
    }
}
```

**Configuration Validation:**
```go
func validateConfig(config KafkaConfig) error {
    if config.Producer.Flush.Bytes > config.Producer.Flush.Max {
        return errors.New("flush bytes cannot exceed max")
    }
    if config.Consumer.Fetch.Default > config.Consumer.Fetch.Max {
        return errors.New("fetch default cannot exceed max")
    }
    return nil
}
```

## üß™ Hands-on Experiments

### Experiment 1: Throughput Optimization

**Goal**: Maximize message throughput

**Steps**:
1. Create baseline producer configuration
2. Measure throughput with different batch sizes
3. Test different compression algorithms
4. Optimize acknowledgment settings
5. Measure final throughput

**Expected Results**:
- Larger batches increase throughput
- Compression reduces network usage
- acks=1 provides good balance

### Experiment 2: Latency Optimization

**Goal**: Minimize message latency

**Steps**:
1. Create baseline consumer configuration
2. Reduce fetch wait times
3. Optimize processing pipeline
4. Use async processing
5. Measure latency improvements

**Expected Results**:
- Smaller batches reduce latency
- Async processing improves throughput
- Optimized fetch settings reduce wait time

### Experiment 3: Resource Utilization

**Goal**: Optimize resource usage

**Steps**:
1. Monitor baseline resource usage
2. Tune memory settings
3. Optimize CPU usage
4. Configure disk I/O
5. Measure resource efficiency

**Expected Results**:
- Balanced memory usage
- Efficient CPU utilization
- Optimized disk I/O patterns

### Experiment 4: Scaling Test

**Goal**: Test horizontal scaling

**Steps**:
1. Start with single broker
2. Add more brokers
3. Increase partition count
4. Add more consumers
5. Measure scaling impact

**Expected Results**:
- Linear scaling with brokers
- Improved parallelism with partitions
- Better load distribution with consumers

## üìä Performance Monitoring

### **Key Performance Indicators (KPIs):**

#### **Throughput Metrics:**
- **Messages per second**: Rate of message processing
- **Bytes per second**: Rate of data processing
- **Partitions per second**: Rate of partition processing

#### **Latency Metrics:**
- **P99 Latency**: 99th percentile latency
- **P95 Latency**: 95th percentile latency
- **Average Latency**: Mean processing time

#### **Resource Metrics:**
- **CPU Utilization**: Percentage of CPU used
- **Memory Usage**: Amount of memory consumed
- **Disk I/O**: Read/write operations per second
- **Network I/O**: Bytes sent/received per second

#### **Quality Metrics:**
- **Error Rate**: Percentage of failed operations
- **Consumer Lag**: Messages behind latest offset
- **Availability**: System uptime percentage

### **Monitoring Implementation:**
```go
type PerformanceMonitor struct {
    Throughput    float64
    Latency       time.Duration
    CPUUsage      float64
    MemoryUsage   float64
    ErrorRate     float64
    ConsumerLag   int64
}

func (m *PerformanceMonitor) CollectMetrics() {
    // Collect all performance metrics
    m.Throughput = calculateThroughput()
    m.Latency = calculateLatency()
    m.CPUUsage = getCPUUsage()
    m.MemoryUsage = getMemoryUsage()
    m.ErrorRate = calculateErrorRate()
    m.ConsumerLag = getConsumerLag()
}

func (m *PerformanceMonitor) GenerateReport() PerformanceReport {
    return PerformanceReport{
        Timestamp: time.Now(),
        Metrics:   *m,
        Health:    m.calculateHealth(),
        Alerts:    m.generateAlerts(),
    }
}
```

## üéØ Key Takeaways

1. **Performance tuning is iterative** - measure, optimize, repeat
2. **Throughput and latency trade off** - optimize for your use case
3. **Resource utilization matters** - balance CPU, memory, disk, network
4. **Scaling strategies** - horizontal vs vertical scaling
5. **Monitoring is essential** - track KPIs and set alerts
6. **Configuration management** - environment-specific settings
7. **Testing is crucial** - benchmark before and after changes

## üìù Lesson 8 Assessment Questions

1. **What is the systematic approach to performance tuning?**
2. **How do you optimize for throughput vs latency?**
3. **What are the key resource utilization factors?**
4. **How do you plan capacity for scaling?**
5. **What metrics should you monitor in production?**
6. **How do you benchmark Kafka performance?**
7. **What are the trade-offs of different scaling strategies?**
8. **How do you validate performance improvements?**

---

## üîÑ Next Lesson Preview: Error Handling and Resilience

**What we'll learn**:
- Error handling patterns and strategies
- Retry mechanisms and circuit breakers
- Dead letter queues and error recovery
- Monitoring and alerting for errors
- Resilience testing and chaos engineering

**Hands-on experiments**:
- Implement retry mechanisms
- Set up dead letter queues
- Test circuit breaker patterns
- Monitor error rates and recovery
